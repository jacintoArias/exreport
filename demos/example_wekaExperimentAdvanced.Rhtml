<html>

<head>
<title>Machine Learning Weka Advanced</title>
</head>

<body>

<h3>Introduction</h3>

<p>
In this example we make use of the wekaExperiment dataset that is packaged with
exreport. This dataset represents a series of experiment in which several Machine
Learning classifiers are compared by using a set of public datasets (UCI repository)
as benchmark. The performance measures are the classification accuracy and the
training time and have been obtained from a 10-fold cross-validation.
</p>

<p>
The <i>wekaExperiment</i> contains a variable for the method and dataset and two
# variables for the respective outputs. The datasets contains two parameters, 
# the mentioned fold of the cross-validation for each entry and an additional 
# boolean parameter to indicate if feature selection has been performed or not
# in the execution. 
</p>

<!--begin.rcode
library(exreport)
colnames(wekaExperiment)
end.rcode-->


<h3>Initial experimentation</h3>

<p>
In this example we simulate an extension in our experimentation using more
datasets and values for input parameters. In order to do that, we assume that
first we use only 5 datasets, and fold = 0. Also, we do not take into account
the feature selection, so it is not a parameter for us (but its value is "no").
After that, we extend the experimentation using 10 more datasets, ten folds
(from 0 to 9) and two featureSelection  values ("yes" and "no").
Finally, we compare the performace regarding accuracy of the different methods.
</p>

<p>
We use data from the wekaExperiment for the first experimentation and the later
extension. For that, we first need to load the whole experimentation (called 
weka extension) and apply a subset to simulate the first experimentation (weka).
</p>

<!--begin.rcode fig.width=7, fig.height=6
experiment <- expCreate(wekaExperiment, name="weka", parameters=c("fold"))
problems   <- c("anneal", "audiology", "balance-scale", "car", "glass")
experiment <- expSubset(experiment, list(problem = problems,
                                         featureSelection = "no",
                                         fold = 0) )
end.rcode-->

<p>
Even if the subset has been done, the experiment object remember the possible
values for the problem variable. Because of that, if we see the output of
summary(experiment), all the problems are there. However, if we call 
table(experiment$data$problem), we can see only the selected problems are in data
</p>

<!--begin.rcode fig.width=7, fig.height=6
summary(experiment)
table(experiment$data$problem)
end.rcode-->


<p>
It is common that experiment results are in tabular representation (the methods
in rows, the problems in columns and the value in the i-j position represents
the output of the method i for the problem j). In this case, we will use that
representation and load the experiment for that (using the data in experiment).
As we have two outputs (accuracy and trainingTime) we need two tables.
Note that this is only a didactic example, as we already have the experiment
object that we want in the experiment variable.
</p

<!--begin.rcode fig.width=7, fig.height=6
subExp   <- experiment$data[experiment$data$problem %in% problems,]
accuracy <- reshape2::dcast(subExp[,-4], method ~ problem, value.var="accuracy")
accuracy
time <- reshape2::dcast(subExp[,-3], method ~ problem, value.var="trainingTime")
time
end.rcode-->


<p>
At this point we have two tabular data, representing the outputs for 4 methods
and 5 datasets, using the fold value 0 (and no feature selection parameter is
used, although we know its value is "no").
Now, we load two experiments and combine them into a single one.
</p>

<!--begin.rcode fig.width=7, fig.height=6
experimentAcc  <- expCreateFromTable(accuracy, "accuracy", "wekaAccuracy", 
                                     parameters = list(fold=0))
experimentTime <- expCreateFromTable(time, "trainingTime", "wekaTrainingTime", 
                                     parameters = list(fold=0))
experiment     <- expCombine(experimentAcc, experimentTime, name = "weka")
summary(experiment)
end.rcode-->


<p>
In this example, we will build the report dynamically. So we create a new report
here at the begining, and we will add each element when it is created.
</p>

<!--begin.rcode fig.width=7, fig.height=6
report <- exreport("Your wekaExperimentAdvanced example report")
# We add the experiment to the report
report <- exreportAdd(report, experiment)
end.rcode-->

<h3>Extending the experimentation</h3>

<p>
Now, we want to extend the experimentation using 15 datasets, 10 folds (from 0
to 9) and two values for the parameter featureSelection ("yes" and "no").
However, suppose during the experimentation an error occurred, and for the fold 0 and
the dataset "anneal" the results were not obtained. For that, we want to
combine both experiments into a single one, and remove the duplicated instances.
</p>

<!--begin.rcode fig.width=7, fig.height=6
wekaExperiment2         <- wekaExperiment[wekaExperiment$problem!="anneal" |
                                            wekaExperiment$problem!=0,]
experimentExt           <- expCreate(wekaExperiment2, name="weka extension",
                                     parameters=c("featureSelection","fold"))
end.rcode-->

<p>
Right now, the concatenation of the two experiments will rise an error. This
is because experiment has not featureSelection parameter
</p>

<!--begin.rcode fig.width=7, fig.height=6
summary(experiment)
expConcat(experiment, experimentExt) # An error is rised
end.rcode-->

<p>
The experiment does not have the featureSelection parameter (we know its value
is "no", but as we did not take into account that variable, it does not
appear). Therefore, we need to extend the experiment adding this variable.
</p>

<!--begin.rcode fig.width=7, fig.height=6
experiment <- expExtend(experiment, list(featureSelection="no"))
summary(experiment)
# Now we can concat the two experiments (but there are duplicated instances).
experimentExt <- expConcat(experiment, experimentExt) # A warning is rised
# Now we can get the duplicated instances. Also, we can remove them.
head(expGetDuplicated(experimentExt)$data)
experimentExt <- expRemoveDuplicated(experimentExt)
# We add the experiment to the report
report <- exreportAdd(report, experimentExt)
end.rcode-->

<h3>Preprocessing data</h3>

<p>
Before proceeding with any kind of analysis we must preprocess our results by
performing the appropiate operations. We will begin by aggregating the result
of the 10-fold cross validation performed for each method and dataset.
For that, we reduce the fold parameter by using the mean function: 
</p>

<!--begin.rcode fig.width=7, fig.height=6
experimentExt <- expReduce(experimentExt, "fold", FUN = mean)
summary(experimentExt)
end.rcode-->

<p>
Now suppose we want to rename some methods. We can do that using expRename.
</p>

<!--begin.rcode fig.width=7, fig.height=6
experimentExt <- expRename(experimentExt, name="weka extension 2", list(method = 
                                                                          list("NaiveBayes"="NBayes",
                                                                               "RandomForest"="RndFrst"),
                                                                        featureSelection=
                                                                          list("no"="F", "yes"="T")))
summary(experimentExt)
# We add the experiment to the report
report <- exreportAdd(report, experimentExt)
end.rcode-->

<p>
Now that we have a single configuration we instantiate the methods with the
available paramaters. 
</p>

<!--begin.rcode fig.width=7, fig.height=6
experimentExt <- expInstantiate(experimentExt)
summary(experimentExt) # Parameters indicate which are instantiated
end.rcode-->

<h3>Visualization</h3>

<p>
We can visualize the experiment
</p>
<!--begin.rcode fig.width=7, fig.height=6
plot <- plotExpSummary(experimentExt, "accuracy", columns = 3)
plot
# We add the plot to the report
report <- exreportAdd(report, plot)
end.rcode-->

<p>
Possibly, we want a different order for the methods or problems.
In this case, we want to compare results for methods when feature selecction
is applied or not, so we want "J48,yes" after "J48,no" and so on.
</p>
<!--begin.rcode fig.width=7, fig.height=6
experimentExt <- expReorder(experimentExt, list(method=c("J48,F", "J48,T",
                                                         "NBayes,F","NBayes,T",
                                                         "OneR,F","OneR,T",
                                                         "RndFrst,F","RndFrst,T")))
summary(experimentExt)
end.rcode-->

<p>
If we visualize the experiment again, we have the desired plot
</p>
<!--begin.rcode fig.width=7, fig.height=6
plot <- plotExpSummary(experimentExt, "accuracy", columns = 3)
plot
# We add the plot to the report
report <- exreportAdd(report, plot)
end.rcode-->

<h3>Validation</h3>
<p>
Our experiment is now ready to be validated by using statistical tests. In this
case we have more than two methods, so we will choose a multiple comparison test.
We want to compare all pairs of methods, for that we will perform a
"testMultiplePairwise" test, including a Friedman test followed by a post-hoc
test using the Holm procedure. As the target variable is the accuracy,
the test will measure maximization.
</p>

<!--begin.rcode fig.width=7, fig.height=6
testAccuracyPairwise <- testMultiplePairwise(experimentExt,"accuracy", "max")
summary(testAccuracyPairwise)
end.rcode-->

<p>
Now we generate a tabular summary for the pairwise test. 
</p>

<!--begin.rcode fig.width=7, fig.height=6
table <- tabularTestPairwise(testAccuracyPairwise, "pvalue")
table
# We add the table to the report
report <- exreportAdd(report, table)
end.rcode-->

<p>
We also want to make a double test pipeline. First, we want to obtain a ranking
among the methods and decide which one is the best one, based on accuracy.
After that, for all those methods which are not statistically different from
the best one, we will perform a second test (again, based in rankings) but in
this case based on training time (therefore the test will measure minimization).
The alpha value used in this case is 0.32. This value has been chosen just for
didactic reasons, in order to get only one method not statistically different
from the best one.
</p>

<!--begin.rcode fig.width=7, fig.height=6
testAccuracy    <- testMultipleControl(experimentExt, "accuracy", "max", alpha = 0.32)
summary(testAccuracy)
# We add the test to the report
report <- exreportAdd(report, testAccuracy)

# Now we generate a tabular summary for the pairwise test. 
table <- tabularTestSummary(testAccuracy, "pvalue")
table
# We add the table to the report
report <- exreportAdd(report, table)

# Here we extract the instances from the methods which are not statistically
# differente from the best one
experimentExt2  <-expExtract(testAccuracy)
summary(experimentExt2)
end.rcode-->

<p>
Now we want to apply a second level test based on training time. In this case,
the previous function testMultipleControl will arise an error as only two
methods passed the test (in that case, a paired test is needed).
</p>

<!--begin.rcode fig.width=7, fig.height=6
testTime <- testMultipleControl(experimentExt2, "trainingTime", "min")
end.rcode-->

<p>
We use a paired test in order to make de comparison.
</p>

  <!--begin.rcode fig.width=7, fig.height=6  
testTime <- testPaired(experimentExt2, "trainingTime", "min")
summary(testTime)
# We add the test to the report
report <- exreportAdd(report, testTime)
end.rcode-->

<h3>Communication</h3>
<p>
Now the report has been built. Now we render it to visualize.
</p>

<!--begin.rcode fig.width=7, fig.height=6
# Render the report in HTML format:
exreportRender(report, target = "html", visualize = T)
# Render the report in pdf format:
exreportRender(report, target = "pdf", visualize = T)
end.rcode-->

</body>
</html>
